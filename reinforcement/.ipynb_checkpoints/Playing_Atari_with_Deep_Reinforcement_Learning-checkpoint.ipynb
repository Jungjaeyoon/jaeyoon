{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing Atari with Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## challenges of Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Learning control agent from high-dimensional sensory inputs(vision, speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep learning 관점 Reinforcement learning의 한계점들"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Most successful deep learning applications to date have required large amounts of hand-labelled training data. \n",
    "- ### Most deep learning algorithms assume the data samples to be independen\n",
    "- ### Deep learning methods that assume a fixed underlying distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제가 되는 이유는..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### RL algorithms must be able to learn from a scalar reward signal that is frequently sparse, noisy and delayed\n",
    "- ###  reinforcement learning one typically encounters sequences of highly correlated state\n",
    "- ### data distribution changes as the algorithm learns new behaviours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional neural network can overcome these challenges to learn successful  control  policies  from  raw  video  data  in  complex  RL  environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Goal :** Arcade Learning Environment (ALE) 에서 구현한 Atari 게임들을 인간과 마찬가지로 비디오 입력, 보상 및 터미널 신호, 가능한 동작을 학습하여 플레이하는 단일 신경망을 구성하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Agent interacts with an environment $\\epsilon$\n",
    "    - ###  Atari emulator, in a sequence of actions, observations and rewards.\n",
    "    - ### In each time step agent selects an action $a_t$ from set of actions $A = \\{1, ... , K\\}$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### agent는 emulator의 내부 프로세스를 관측하는 것이 아닌 image $ x_t \\in R^d $ 를 관측한다\n",
    "   - ### $x_t$ is vector of raw pixel values representing the current screen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward $ r^t$ representing change in game score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### agent는 이미지만 관찰하기 때문에 emulator의 많은 상태가  perceptually aliased임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### $x_t$ 만으로 현재 상황을 모두 이해할 수 없기 때문에 전체 과정인 $s_t = \\{x_1,a_1,x_2,...,a_{t-1},x_t\\}$를 고려하며 학습해야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경과 상태 에이전트 리워드가 모여 MDP를 구성함\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " - ### MDP를 정의하고 MDP를 풀기 위한 일반적인 강화 학습 기법을 적용할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### agent의 목표 - future rewards를 최대화하는 행동을 하는 것\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### 감가율 $\\gamma$를 적용한 reward는 $R_t = \\sum^T_{t'=t}\\gamma^{t'-t}r_t'$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### 앞으로 받을 보상합을 최대로 해주는 큐함수에 대한 벨만 최적 방정식\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$q_*(s,a) = E\\left[R_{t+1}+\\gamma max_{a'}q_*(S_{t+1},a')|S_t=s, A_t=a \\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow2",
   "language": "python",
   "name": "tensorflow2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
