{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 강화 학습이란?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### 강화학습은 보상을 통해 학습한다\n",
    "    - #### 기본 개념\n",
    "        - **에이전트:** 스스로 학습하는 컴퓨터\n",
    "        - **환경**\n",
    "        - **보상:** 컴퓨터가 선택한 학습에 대한 환경의 반응\n",
    "* #### 강화학습은 에이전트가 환경을 탐색하며 얻는 보상합을 최대화하는 행동 양식이나 정책을 학습한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 강화 학습의 특징"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### 환경에 대한 정확한 사전지식을 요구하지 않는다\n",
    "- #### 시행착오를 통해 학습\n",
    "- #### 강화학습은 순차적 결정을 요구하는 문제에 적용된다\n",
    "    *  순차적으로 행동을 결정하는 문제를 정의할 때 __MDP(Markov Decision Process)__ 를 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDP "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 다음과 같은 구성 요소들을 가진다\n",
    "- **상태**\n",
    "- **행동**\n",
    "- **보상 함수**\n",
    "- **상태 변환 확률**\n",
    "- **감가율**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 상태\n",
    "- 에이전트가 관찰 가능한 상태의 집합\n",
    "$$S = \\{(x_1,y_1),(x_2,y_2),(x_3,y_3),(x_4,y_4),(x_5,y_5)\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 행동\n",
    "- 에이전트가 특정 상태에서 할 수 있는 행동\n",
    "    - 예시: 벽돌 깨기에서 강 상태에서 취할 수 있는 행동은 {left, right}\n",
    "\n",
    "$$A = \\{Action_1, Action_2, ... , Action_n\\}$$\n",
    "- 특정 시험 t에서 행동들의 집합 A에서 취한 행동\n",
    "$$A_t = a$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 보상 함수\n",
    "- 에이전트가 학습할 수 있는 유일한 정보\n",
    "    - 특정 시간과 상태에서 행동 $a$를 했을 때 받을 보상에 대한 기댓값\n",
    "    - 행동과 상태가 항상 같은 결과를 보장하지는 않는다\n",
    "        - 환경의 차이가 주는 변화가 있을 수 있기 때문에\n",
    "    - 시점 $t$에 수행된 행동의 보상은 시점 $t+1$일 때 획득 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathrm{R}_{s}^{a} = E[R_{t+1} | S_t = s, A_t = a]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  4. 상태 변환 확률\n",
    "- 에이전트가 상태 $s$에서 행동 $a$를 거쳐 $s'$으로 가게 되는 확률\n",
    "    - 왜 확률적 요소가 들어갈까?\n",
    "        - 행동을 취했을 때 예상치 못한 요소(ex: 등교길에 갑자기 비가 내리거나 넘어지는 등)가 존재할 수 있기 때문에 확률적인 요인을 고려\n",
    "    - 상태 변환 확률값 또한 환경의 일부이다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathrm{P}_{ss'}^{a} = P[S_{t+1} = s' | S_t = s, A_t = a]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. 감가율\n",
    "- 현재에 가까운 보상일수록 큰 가치를 지님\n",
    "    - 시간이 지날수록 감소하는 보상의 가치를 표현하는 개념\n",
    "    - 미래 보상을 계산할 때 감가율을 고려하여야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\gamma^{k-1}R_{t+k} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정책"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모든 상태에서 에이전트가 할 행동, 함수\n",
    "- 각 상태에 대응하는 하나의 행동이 되거나 $a_1 = 10$%$, a_2 = 20$% 와 같이 확률로도 표현할 수 있음\n",
    "- 시간 $t$, 상태 $S_t=s$에 에이전트가 있을 때 가능한 행동 중 $A_t = a$ 를 수행할 확률\n",
    "\n",
    "#### **에이전트는 강화학습을 통해 최적 정책을 학습한다**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 에이전트의 행동 -> 환경의 보상이 서로 상호작용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\pi(a | s) = P[A_t = a | S_t = s ]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 가치함수\n",
    "- 앞에서 에이전트는 MDP를 통해 문제를 학습한다고 설명. MDP를 통해 최적 정책을 찾을 수 있음\n",
    "- 최적 정책을 찾는 방법은? **가치함수**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ R_{t+1} + R_{t+2} + R_{t+3} + R_{t+4} + ...  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>단순 보상합을 할 경우 현재와 미래의 보상을 똑같이 취급하기 때문에 여러 문제가 발생한다 </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$G_t =  R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\gamma^3 R_{t+4} + ...  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> G(retrurn)은 감가율을 적용한 보상값의 합을 의미한다</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 앞으로 얻을 보상에 대해 예측할 수 있지 않을까?\n",
    "    - 특정 상태에 있을 때 앞으로 어떤 보상을 받을 것인지에 대한 기댓값을 표현한 것이 **가치 함수**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ v(s) = E[G_t|S_t=s] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ v(s) = E[ R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\gamma^3 R_{t+4} + ...|S_t=s] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>가치 함수 - 앞으로 받을 보상에 대한 기댓값</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$v(s) = E[ R_{t+1} + \\gamma(R_{t+2} + \\gamma^1 R_{t+3} + \\gamma^2 R_{t+4} + ...)|S_t=s]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$v(s) = E[ R_{t+1} + \\gamma G_{t+1}|S_t=s]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 가치함수는 모든 상태에 대해 계산된다\n",
    "- 강 상태에서 행동을 하기위해서 정책을 따라야 한다\n",
    "- 정책을 따름을 파이로 표현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$v_\\pi(s) = E_\\pi\\left[ R_{t+1} + \\gamma v_\\pi(S_{t+1})|S_t=s\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Bellman Expectation Equation - 현재 상태의 가치함수와 다음 상태의 가치함수의 관계를 말해준다</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 가치 함수를 통해 다음에 어떤 상태로 가야할 지 판단할 수 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$v_\\pi(s) =\\sum_{a \\in A} \\pi(a | s)\\left(R_{t+1} + \\gamma \\sum_{s' \\in S}P^a_{ss'}v_\\pi(s')\\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>계산할 수 있는 형태의 가치함수</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 계속 진행하다 보면 언젠가 왼쪽 항과 오른쪽 항이 동일해짐(그럴까요)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 큐 함수 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 가치 함수는 상태를 입력으로, 앞으로 받을 보상합을 출력으로 한다.\n",
    "- 하지만 특정 상태에서 각 행동의 결과에 대해 따로 가치함수를 만든다면 굳이 다음 상태의 가치함수를 계산할 필요 없이 행동을 결정할 수 있을 것\n",
    "- 어떤 상태에서 어떤 행동이 얼마나 좋은지 알려주는 함수를 행동 가치함수 - 큐함수(Q Function)이라고 한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ q_\\pi(s,a) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> 큐함수는 상태, 행동 두 가지 변수를 지닌다</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ v_\\pi(s) = \\sum_{a \\in A} \\pi(a | s)q_\\pi(s,a) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>큐함수와 가치함수의 관계식</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ q_\\pi(s,a) = E_\\pi[R_{t+1} + \\gamma q_\\pi(S_{t+1},A_{t+1}) | S_t = s, A_t = a] $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모든 행동에 대해 -> 수행할 확률 X 행동을 했을 때 받을 보상인 큐함수를 합하면  상태 가치함수가 된다?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 벨만 최적 방정식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 벨만 기대 방정식은 상태 s에서 앞으로 받을 보상에 대한 기댓값을 알려준다\n",
    "    - 계산할 수 있는 형태의 가치함수가 결국 수렴하기 때문에 결과적으로 현재 정책에 대한 참 가치함수를 구하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그렇다면 수많은 정책 중 가장 높은 보상을 주는 **최적 가치함수**는 어떻게 구할 까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$v_{k+1}(s) =\\sum_{a \\in A} \\pi(a | s)\\left(R^a_s + \\gamma v_k(s')\\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$v_{k+1}(s) =\\max_{a \\in A}\\left(R^a_s + \\gamma v_k(s')\\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 식에서 k+1은 k+1번째 가치함수를 의미하며 k번째 가치함수 중 주변 상태 s'을 이용해 구한다\n",
    "    - 주변 상태에 저장된 가치함수를 통해 현재의 가치함수를 업데이트 할 수 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ v_*(s) = \\max_\\pi\\left[v_\\pi(s)\\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>최적의 가치함수</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$q*(s,a) = \\max_\\pi\\left[q_\\pi(s,a)\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>최적의 큐함수</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 최적의 큐함수(또는 가치함수)를 찾았다면, 최적 정책은 최적의 큐함수에서 가장 큰 큐함수(보상)을 가진 행동을 하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\pi_*(s,a) =\\begin{cases} \n",
    " & \\text{ 1 } if a = argmax_{a \\in A}q*(s,a)  \\\\ \n",
    " & \\text{ 0 } \\qquad otherwise\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 각 행동을 선택할 때 큐함수를 최대로 해주는 행동을 반환하며 최적 정책을 따라간다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 최적의 가치함수(또는 큐함수)는 그렇다면 어떻게 찾을 수 있을까?\n",
    "    - 벨만 방정식은 -> 현재 상태의 가치함수와 다음 시점의 가치함수 사이의 관계식임\n",
    "    - 현재 상태의 가치함수가 최적이라면, 에이전트는 가장 좋은 행동을 선택할 것.\n",
    "        - 가장 좋은 행동은 큐함수를 통해 알 수 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ v_*(s) = \\max_a\\left[q_*(s,a)|S_t=s, A_t=a \\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>큐함수 중 최대를 선택하는 최적 가치함수</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ v_*(s) =\\max_aE\\left[R_{t+1} + \\gamma v_*(S_{t+1})|S_t=s,A_t=a\\right]  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>벨만 최적 방정식 - 최적의 행동을 했을 때의 가치함수</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$q_*(s,a) = E\\left[R_{t+1}+\\gamma max_{a'}q*(S_{t+1},a')|S_t=s, A_t=a \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>벨만 최적 방정식 - 큐함수에 대한</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow2",
   "language": "python",
   "name": "tensorflow2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
