{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### 다이내믹 프로그래밍으로 벨만 방정식을 푼다 - 정책 이터레이션 \n",
    "- ### 다이내믹 프로그래밍으로 벨만 최적 방정식을 푼다 - 가치 이터레이션"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그리드월드 예제를 통해 실습해보기 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 순차적 행동 문제를 풀기 위해서\n",
    "#### - MDP 정의\n",
    "#### - 벨만 방정식 계산\n",
    "    -   벨만 기대 방정식\n",
    "    -   벨만 최적 방정식\n",
    "#### - 최적 가치함수 & 정책 찾기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 벨만 방정식을 푼다?\n",
    "    - 벨만 최적 방정식을 만족하는 v* 을 찾는 것!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ v_*(s) =\\max_aE\\left[R_{t+1} + \\gamma v_*(S_{t+1})|S_t=s,A_t=a\\right]  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다이내믹 프로그래밍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 강화 학습보다 먼저 존재한 벨만 방정식을 푸는 알고리즘 !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 큰 문제를 작은 문제로 쪼개서 불어보자\n",
    "    - 시간에 따라 프로세스들을 풀어가는 과정\n",
    "    - 프로세스를 이용해 다른 프로세스를 풀어갈 수 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ v_pi(s) -> v_pi(s_1), v_pi(s_2), v_pi(s_3) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정책 이터레이션 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 정책을 평가하고 발전시키는 과정을 반복하며 최적 정책으로 수렴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 어떻게 가장 높은 보상을 주는 정책을 찾을 수 있을까?\n",
    "    - 어떤 정책이 좋은 정책일까? - 벨만 기대 방정식을 사용한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$v_\\pi(s) =\\sum_{a \\in A} \\pi(a | s)\\left(R_{t+1} + \\gamma \\sum_{s' \\in S}P^a_{ss'}v_\\pi(s')\\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정책 평가 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. k번째 가치함수 행렬에서 현재 상태 s에서 갈 수 있는 다음 상태 s'에 저장된 가치함수를 불러온다\n",
    "$$v_k(s')$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 감가율을 적용하고 보상을 더한다\n",
    "$$ R^a_s + \\gamma v_k(s') $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 정책을 적용한다 -- 확률값을 곱한다\n",
    "$$ \\pi(a|s)(R^a_s + \\gamma v_k(s')) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 모든 가능한 행동에 대해 반복하고 값을 더한다\n",
    "$$ \\sum_{a \\in A}\\pi(a|s)(R^a_s + \\gamma v_k(s')) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. k+1번째 가치함수의 상태 s에 값을 저장한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. 모든 s에 대해 반복"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 이 과정을 통해 한 번의 정책평가를 마침 / 무한한 반복을 통해 참 v_pi를 찾을 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정책 발전"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 평가를 바탕으로 정책을 발전시킨다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 방법 중 1 - greedy policy improvement\n",
    "#### 선택할 수 있는 행동 중 가장 큰 큐함수를 가지는 행동을 선택한다\n",
    "    눈앞에 보이는 것 중 당장 큰 이익을 추구하는 행동\n",
    "$$ \\pi'(s) = argmax_{a \\in A}q_\\pi(s,a)  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정책 이터레이션은 정책 평가와 발전을 통해 최적 정책을 찾아낼 수 있다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 가치 이터레이션"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정책 이터레이션에서 정책은 확률로 인식할 수 있다 - 때문에 기댓값의 개념을 사용하며 이는 벨만 기대 방정식을 사용하는 이유가 된다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정책이 결정적인(확률이아닌) 형태로 제공된다면? - 가치 이터레이션\n",
    "    - 현재의 가치함수를 최적이라 가정하고 결정적 형태의 정책 적용\n",
    "    - 결과적으로 최적 정책을 찾아낸다면..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **벨만 기대 방정식을 반복적으로 계산하여 얻는 값은 결국 참 가치함수**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ v_*(s) = \\max_a E[R_{t+1} + \\gamma v_*(S_{t+1}) | S_t = s , A_t = a]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ v_{k+1}(s) = \\max_{a \\in A}(R^a_s + \\gamma v_k(s')) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 정책이 결정적(주어진 정책이 최적 정책임을 가정)이기 때문에 정책 평가 과정을 거치면 최적 가치함수와 정책을 구할 수 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  - 그리드월드 예제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 다이내믹 프로그래밍은 환경에 대한 모든 정보를 알고 문제에 접근한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/rlcode/reinforcement-learning-kr "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다이내믹 프로그래밍의 한계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- '계산' 이지 '학습'이 아니다\n",
    "- 다이내믹 프로그래밍의 문제\n",
    "    - 계산 복잡도(n ** 3)\n",
    "    - 차원의 저주\n",
    "    - 환경에 대한 정보 필요\n",
    "        - 실제 활용할 때 환경을 전부 알 수 있을까?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow2",
   "language": "python",
   "name": "tensorflow2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
